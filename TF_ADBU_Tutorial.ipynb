{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_ADBU_Tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByDU2E7r-Xpj",
        "colab_type": "text"
      },
      "source": [
        "# **<font color=”brown”> Hands-on Tutorial on Tensorflow </font>**\n",
        "\n",
        "\n",
        "## **Session 4: Learning Tensorflow for Developing Machine Learning Models**\n",
        "\n",
        "## **<font color=”ec0909”> Workshop on Signal Processing and Machine Learning, Assam Don Bosco University</font>**\n",
        "\n",
        "## **<font color=”068632”>Resource Person: Sukanta Dey, IIT Guwahati</font>**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "NOTE: You have to run all the code cells chronogically. If you don't do so, you may face error while you execute some depenent python commands which was defined in previous cells.\n",
        "\n",
        "# **Multiclass Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff513sU-V4vz",
        "colab_type": "text"
      },
      "source": [
        "The dataset for the classification example can be downloaded freely from this [link](https://raw.githubusercontent.com/thesukantadey/TF_ADBU/master/data/car_evaluation.csv). Download the file in CSV format. If you open the downloaded CSV file, you will see that the file doesn't contain any headers. The detail of the columns is available at UCI machine learning repository. I will recommend that you read the dataset information in detail from the download link. I will briefly summarize the dataset in this section.\n",
        "\n",
        "The dataset basically consists of 7 columns:\n",
        "\n",
        "    price (the buying price of the car)\n",
        "    maint ( the maintenance cost)\n",
        "    doors (number of doors)\n",
        "    persons (the seating capacity)\n",
        "    lug_capacity (the luggage capacity)\n",
        "    safety (how safe is the car)\n",
        "    output (the condition of the car)\n",
        "\n",
        "Given the first 6 columns, the task is to predict the value for the 7th column i.e. the output. The output column can have one of the four values i.e. \"unacc\" (unacceptable), \"acc\" (acceptable), good, and very good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBtM8-4OV_kJ",
        "colab_type": "text"
      },
      "source": [
        "Importing Libraries\n",
        "\n",
        "Before we import the dataset into our application, we need to import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYKmBl0p4bB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Python example\n",
        "1+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fax8Bmk7zt6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swlvWvAtz7q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0t6Me0MWJRy",
        "colab_type": "text"
      },
      "source": [
        "The following script imports the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjWRVGg70Ist",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/thesukantadey/TF_ADBU/master/data/car_evaluation.csv'\n",
        "\n",
        "\n",
        "cols = ['price', 'maint', 'doors', 'persons', 'lug_capacity', 'safety','output']\n",
        "cars = pd.read_csv(url, names=cols)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIdAbaK_oovb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(cars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyX7qfJc43Ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cars.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjVVuW4v5YAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_size = plt.rcParams[\"figure.figsize\"]\n",
        "plot_size [0] = 8\n",
        "plot_size [1] = 6\n",
        "plt.rcParams[\"figure.figsize\"] = plot_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NltEILQb5flr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cars.output.value_counts().plot(kind='pie', autopct='%0.05f%%', colors=['lightblue', 'lightgreen', 'orange', 'pink'], explode=(0.05, 0.05, 0.05,0.05))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOOeZeHw5w5s",
        "colab_type": "text"
      },
      "source": [
        "The output shows that majority of cars (70%) are in unacceptable condition while 20% cars are in acceptable conditions. The ratio of cars in good and very good condition is very low.\n",
        "\n",
        "All the columns in our dataset are categorical. Deep learning is based on statistical algorithms and statistical algorithms work with numbers. Therefore, we need to convert the categorical information into numeric columns. There are various approaches to do that but one of the most common approach is one-hot encoding. In one-hot encoding, for each unique value in the categorical column, a new column is created. For the rows in the actual column where the unique value existed, a 1 is added to the corresponding row of the column created for that particular value. This might sound complex but the following example will make it clear.\n",
        "\n",
        "The following script converts categorical columns into numeric columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQFUM2Fm519v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "price = pd.get_dummies(cars.price, prefix='price')\n",
        "print(price)\n",
        "maint = pd.get_dummies(cars.maint, prefix='maint')\n",
        "\n",
        "doors = pd.get_dummies(cars.doors, prefix='doors')\n",
        "persons = pd.get_dummies(cars.persons, prefix='persons')\n",
        "\n",
        "lug_capacity = pd.get_dummies(cars.lug_capacity, prefix='lug_capacity')\n",
        "safety = pd.get_dummies(cars.safety, prefix='safety')\n",
        "\n",
        "labels = pd.get_dummies(cars.output, prefix='condition')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8jW2cJE57V2",
        "colab_type": "text"
      },
      "source": [
        "To create our feature set, we can merge the first six columns horizontally:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm9QGFXo58Qk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = pd.concat([price, maint, doors, persons, lug_capacity, safety] , axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC_y2ZNGplAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQCq4lkh5_EF",
        "colab_type": "text"
      },
      "source": [
        "Let's see how our label column looks now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAKiZ8Ui6E3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csNbFpkD6K3-",
        "colab_type": "text"
      },
      "source": [
        "The label column is basically a one-hot encoded version of the output column that we had in our dataset. The output column had four unique values: unacc, acc, good and very good. In the one-hot encoded label dataset, you can see four columns, one for each of the unique values in the output column. You can see 1 in the column for the unique value that originally existed in that row. For instance, in the first five rows of the output column, the column value was unacc. In the labels column, you can see 1 in the first five rows of the condition_unacc column.\n",
        "\n",
        "Let's now convert our labels into a numpy array since deep learning models in TensorFlow accept numpy array as input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLJLCgH06L_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYMgKkhv0BJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-52ipGdJ6Wsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Activation,Dropout\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqWel0dorIaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNIP_5PurUjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMK4h3V92PZ4",
        "colab_type": "text"
      },
      "source": [
        "Dummy NN Example\n",
        "\n",
        "![alt text](https://cloud.githubusercontent.com/assets/1584365/26314676/4f8eb83c-3f41-11e7-9183-2406c7a8759e.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBiUQxwC6aat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Neural Network Structure for classification\n",
        "input_layer = Input(shape=(X.shape[1],))\n",
        "dense_layer_1 = Dense(15, activation='relu')(input_layer)\n",
        "dense_layer_2 = Dense(10, activation='relu')(dense_layer_1)\n",
        "output = Dense(y.shape[1], activation='softmax')(dense_layer_2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSKsNKDE0cGJ",
        "colab_type": "text"
      },
      "source": [
        "Softmax activation function: [[Ref 1]](https://en.wikipedia.org/wiki/Softmax_function)\n",
        "\n",
        "ReLU activation function: [[Ref 2]](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))\n",
        "\n",
        "Categorical Cross Entropy Loss: [[Ref 3]](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy)\n",
        "\n",
        "Adam optimizer: [[Ref 4]](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
        "\n",
        "\n",
        "output_size * (input_size + 1) == number_parameters [[Ref 5]](https://medium.com/@zhang_yang/number-of-parameters-in-dense-and-convolutional-neural-networks-34b54c2ec349)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkG0TGAD0sG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(15*(21+1))\n",
        "print(10*(15+1))\n",
        "print(4*(10+1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai1Ix05r6dgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvX7h8hJ6hw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training\n",
        "history = model.fit(X_train, y_train, batch_size=8, epochs=50, verbose=1, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEAwbsDvKG-u",
        "colab_type": "text"
      },
      "source": [
        "verbose = 1, which includes both progress bar and one line per epoch. verbose = 0, means silent. verbose = 2, one line per epoch i.e. epoch no./total no. of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2bi_mvz6u8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Testing\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "print(\"Test Score:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8d-1cY777fv",
        "colab_type": "text"
      },
      "source": [
        "# **Regression with TensorFlow 2.0**\n",
        "\n",
        "In regression problem, the goal is to predict a continuous value. In this section, you will see how to solve a regression problem with TensorFlow 2.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O08bs_0I7_-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/thesukantadey/TF_ADBU/master/data/petrol_consumption.csv'\n",
        "petrol_cons = pd.read_csv(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMrf1qfv8gKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "petrol_cons.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8WRJZpF8nho",
        "colab_type": "text"
      },
      "source": [
        "You can see that there are five columns in the dataset. The regression model will be trained on the first four columns, i.e. Petrol_tax, Average_income, Paved_Highways, and Population_Driver_License(%). The value for the last column i.e. Petrol_Consumption will be predicted. As you can see that there is no discrete value for the output column, rather the predicted value can be any continuous value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L7xYitG8s7h",
        "colab_type": "text"
      },
      "source": [
        "## **Data Preprocessing**\n",
        "\n",
        "In the data preprocessing step we will simply split the data into features and labels, followed by dividing the data into test and training sets. Finally the data will be normalized. For regression problems in general, and for regression problems with deep learning, it is highly recommended that you normalize your dataset. Finally, since all the columns are numeric, here we do not need to perform one-hot encoding of the columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE04qjkv88HA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = petrol_cons.iloc[:, 0:4].values\n",
        "y = petrol_cons.iloc[:, 4].values\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3lTg_ml9Cgh",
        "colab_type": "text"
      },
      "source": [
        "In the above script, in the feature set X, the first four columns of the dataset are included. In the label set y, only the 5th column is included. Next, the data set is divided into training and test size via the train_test_split method of the sklearn.model_selection module. The value for the test_size attribute is 0.2 which means that the test set will contain 20% of the original data and the training set will consist of the remaining 80% of the original dataset. Finally, the StandardScaler class from the sklearn.preprocessing module is used to scale the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiiSwXK89ew_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDrJKQ6v8tdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVVKnf3N9Dbc",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "input_layer = Input(shape=(X.shape[1],))\n",
        "dense_layer_1 = Dense(100, activation='relu')(input_layer)\n",
        "dense_layer_2 = Dense(50, activation='relu')(dense_layer_1)\n",
        "dense_layer_3 = Dense(25, activation='relu')(dense_layer_2)\n",
        "output = Dense(1)(dense_layer_3)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output)\n",
        "model.compile(loss=\"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In0A49wBJ-rQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5BfqqwY9WGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AClOQwGh9c0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size=2, epochs=100, verbose=1, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xEqeWcV9iZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "pred_train = model.predict(X_train)\n",
        "print(np.sqrt(mean_squared_error(y_train,pred_train)))\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "print(np.sqrt(mean_squared_error(y_test,pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eymPeAqBGOKF",
        "colab_type": "text"
      },
      "source": [
        "# **Question at the end of session: What if the number of neurons in hidden layer is more than the input layer?**\n",
        "\n",
        "\n",
        "**Answer: No significant change in accuracy as shown in the following experiment.Number of nodes in a hidden layer can be arbitrarily decided. There is no hard-and-fast rule. It depends on the type of applications. The number of neurons in hidden layers can be more than the number of neurons in input layer, less than or equal to number of neurons in input layer. Number of nodes is a not a great measure of how problematic a deep network is. Still you can tune the neural network in order to find the optimum number of neurons for your model. That's also comes under hyper-parameter optimization.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baV4DY2XGLDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_layer = Input(shape=(X.shape[1],))\n",
        "dense_layer_1 = Dense(100, activation='relu')(input_layer)\n",
        "dense_layer_2 = Dense(150, activation='relu')(dense_layer_1) #150 neurons more than its previous layer's 100 neuron\n",
        "dense_layer_3 = Dense(25, activation='relu')(dense_layer_2)\n",
        "output = Dense(1)(dense_layer_3)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output)\n",
        "model.compile(loss=\"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0TV1SueGuQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCtyoHRjG0OC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size=2, epochs=100, verbose=1, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeY1QsNfHJBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "pred_train = model.predict(X_train)\n",
        "print(np.sqrt(mean_squared_error(y_train,pred_train)))\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "print(np.sqrt(mean_squared_error(y_test,pred)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}